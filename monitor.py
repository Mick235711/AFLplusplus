#!/usr/bin/env python3

""" Monitor the crash case generated by AFL """

# Libraries
import argparse
import os
import json
import shutil
import subprocess
from pathlib import Path
from timeit import default_timer as timer
from typing import Any, Callable
from watchdog.events import FileSystemEventHandler, FileCreatedEvent, \
    FileSystemEvent
from watchdog.observers import Observer


Runner = Callable[[str, str], tuple[str, float]]
Processor = Callable[[str], dict[str, Any]]


def process_custom(output_dir: str) -> dict[str, Any]:
    """ Process bitmaps """
    os.remove("orig-bitmap.bin")
    os.remove("final-bitmap.bin")
    return {}


def process_output(output: str) -> dict[str, Any]:
    """ Process normal afl-tmin output """
    data: dict[str, Any] = {}
    split_index = output.find("File size reduced by")
    assert split_index != -1, output
    summary = output[split_index:].strip()
    output = output[:split_index].strip()
    for line in output.split("\n"):
        line = line.strip()
        index = line.find(",") + 1
        index2 = -1
        key: str | None = None
        is_multi = True
        if "Block normalization complete" in line:
            index2 = line.find("byte", index)
            key = "bytes_replaced"
        elif "Block removal complete" in line:
            index2 = line.find("byte", index)
            key = "bytes_deleted"
        elif "Symbol minimization finished" in line:
            index2 = line.find("symbol", index)
            key = "symbols_replaced"
        elif "Character minimization done" in line:
            index2 = line.find("byte", index)
            key = "character_bytes_replaced"
        else:
            index = line.find(":") + 1
            index2 = len(line)
            is_multi = False
            if line.startswith("Test case chosen"):
                data["test_case"] = line[index:].strip()
                continue
            elif line.startswith("Original test case length"):
                key = "test_case_size"
            elif line.startswith("Original distance"):
                key = "original_distance"
            elif line.startswith("Optimal distance"):
                key = "optimal_distance"
            elif line.startswith("Success!"):
                index = line.find("to") + 2
                is_multi = True
                key = "distance_trace"
            elif line.startswith("map") and "length = " in line:
                index = line.find("=") + 1
                if "(diff" in line:
                    assert line.endswith(")"), line
                    index2 = line.find("(diff")
                    index3 = line.find("=", index2) + 1
                    data["map_diff"] = int(line[index3:-1].strip())
                key = line[:line.find("length = ")].strip() + "_length"
            elif line.startswith("Total different byte count"):
                key = "map_byte_difference"
            elif line.startswith("abs diff = "):
                for section in line.split(","):
                    assert "diff =" in section, line
                    index = section.find("diff =")
                    key = "map_" + section[:index].strip() + "_diff"
                    data[key] = int(section[section.find("=") + 1:].strip())
                continue
        if key is None:
            continue
        assert index != -1 and index2 != -1, line
        if not is_multi:
            data[key] = int(line[index:index2].strip())
            continue
        if key not in data:
            data[key] = []
        data[key].append(int(line[index:index2].strip()))

    # Process summary
    for line in summary.split("\n"):
        line = line.strip()
        index = line.find(":") + 1
        index2 = len(line)
        key = None
        if line.startswith("Number of execs done"):
            key = "num_exec"
        elif line.startswith("Fruitless execs"):
            index = line.find("path=") + 5
            index2 = line.find("crash=")
            key = "fruitless_exec"
        if key is None:
            continue
        assert index != -1 and index2 != -1, line
        data[key] = int(line[index:index2].strip())

    return data


def read_file(file_path: str) -> str:
    """ Read file contents """
    return str(Path(file_path).read_bytes())[2:-1]


class CrashMonitor(FileSystemEventHandler):
    """ Monitor a crash directory """

    def __init__(
        self, output_dir: str, run_official: Runner, run_custom: Runner,
        store_content: bool = False
    ) -> None:
        """ Constructor """
        super().__init__()
        assert os.path.isdir(output_dir), output_dir
        self.output_dir = output_dir
        self.binary_list: dict[str, tuple[Runner, Processor | None, str]] = {
            "Official afl-tmin": (run_official, None, "official"),
            "Custom afl-tmin": (run_custom, process_custom, "custom")
        }
        self.data: list[dict[str, Any]] = []
        self.store_content = store_content

    def on_created(self, event: FileSystemEvent) -> None:
        """ Event representing file/dir creation """
        if not isinstance(event, FileCreatedEvent):
            return
        self.process(event.src_path)

    def process(self, crash_file: str, export_path: str | None = None) -> None:
        """ Process a crash file """
        assert os.path.isfile(crash_file), crash_file
        print(f"\n===> Detected new crash case: {crash_file}")
        crash_case = os.path.join(self.output_dir, "crash_case")
        crash_reduce = os.path.join(self.output_dir, "crash_reduce")
        shutil.copyfile(crash_file, crash_case)
        if export_path is not None:
            shutil.copyfile(crash_file, export_path)
        crash_case_size = os.path.getsize(crash_case)
        print(f"===> Copied {crash_case_size} bytes to {crash_case}")

        run_data: dict[
            str, tuple[str, float, int, dict[str, Any] | None, str | None]
        ] = {}
        for name, (binary, processor, suffix) in self.binary_list.items():
            print(f"\n===> Running {name.lower()}...")
            output, elapsed = binary(crash_case, crash_reduce)
            print(output)
            if export_path is not None:
                shutil.copyfile(
                    crash_reduce, export_path + "_reduce_" + suffix)
            run_data[name] = (
                output, elapsed, os.path.getsize(crash_reduce),
                None if processor is None else processor(self.output_dir),
                read_file(crash_reduce) if self.store_content else None
            )

        self.data.append({"file_name": os.path.basename(crash_file)})
        if self.store_content:
            self.data[-1]["original"] = read_file(crash_case)
        self.data[-1]["original_size"] = crash_case_size
        self.data[-1]["data"] = {}

        print()
        for name, (
            output, elapsed, reduced_size, data, content
        ) in run_data.items():
            print(f"\n===> {name} statistics:")
            percent = (1 - reduced_size / crash_case_size) * 100
            print(f"===> Final file size: {reduced_size} " +
                  f"({percent:.1f}% reduction)")
            print(f"===> Elapsed time: {elapsed:.2f} seconds")
            self.data[-1]["data"][name] = {}
            if content is not None:
                self.data[-1]["data"][name]["reduced"] = content
            self.data[-1]["data"][name].update({
                "reduced_size": reduced_size,
                "elapsed_seconds": elapsed
            })
            self.data[-1]["data"][name].update(process_output(output))
            if data is not None:
                self.data[-1]["data"][name].update(data)

        if os.path.exists(crash_case):
            os.remove(crash_case)
        if os.path.exists(crash_reduce):
            os.remove(crash_reduce)

    def write_json(self, output_file: str) -> None:
        """ Write data to JSON file """
        output_file = os.path.join(self.output_dir, output_file)
        with open(output_file, "w") as fp:
            json.dump(self.data, fp, indent=4)
        print(f"Results written to {output_file}.")


def run_afl_tmin(
    afl_tmin_binary: str, test_case_dir: str, execute_line: list[str],
    add_d_option: bool = False
) -> Runner:
    """ Run afl-tmin, return the output """
    def runner(crash_case: str, crash_reduce: str) -> tuple[str, float]:
        # Create process
        start = timer()
        process = subprocess.run(
            [afl_tmin_binary, "-i", crash_case, "-o", crash_reduce] + (
                ["-d", test_case_dir] if add_d_option else []
            ) + ["--"] + execute_line,
            universal_newlines=True, text=True,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT
        )
        end = timer()

        # Get process return code
        return_code = process.returncode
        if return_code != 0:
            print(f"Warning: the process returned {return_code}!")

        return process.stdout, end - start
    return runner


def main() -> None:
    """ Main function """
    parser = argparse.ArgumentParser()
    parser.add_argument("-i", "--input-dir",
                        help="AFL crash directory to monitor")
    parser.add_argument("-o", "--output-dir", help="Output directory")
    parser.add_argument("-d", "--test-case-dir", help="Testcase directory")
    parser.add_argument("--official", help="Path to official afl-tmin")
    parser.add_argument("--custom", help="Path to custom afl-tmin")
    parser.add_argument("--store-content", action="store_true",
                        help="Store test case content in JSON output")
    parser.add_argument("--monitor-output", action="store_true",
                        help="Monitor output directory")
    parser.add_argument("--export", required=False, help="Export directory")
    parser.add_argument("binary", nargs="+",
                        help="Testing binary (@@ for filename)")
    args = parser.parse_args()
    assert os.path.isdir(args.input_dir), args.input_dir
    assert os.path.isdir(args.test_case_dir), args.test_case_dir
    assert os.path.isfile(args.official), args.official
    assert os.path.isfile(args.custom), args.custom
    if args.export is not None:
        if os.path.exists(args.export):
            shutil.rmtree(args.export)
        os.makedirs(args.export)
    if os.path.exists(args.output_dir):
        shutil.rmtree(args.output_dir)
    os.makedirs(args.output_dir)

    monitor = CrashMonitor(
        args.output_dir,
        run_afl_tmin(args.official, args.test_case_dir, args.binary),
        run_afl_tmin(args.custom, args.test_case_dir, args.binary, True),
        args.store_content
    )
    print("===> Loading initial crashes...")
    cnt = 0
    for crash_file in os.listdir(args.input_dir):
        cnt += 1
        if crash_file.lower() == "readme.txt":
            continue
        real_path = os.path.join(args.input_dir, crash_file)
        if os.path.isfile(real_path):
            monitor.process(
                real_path,
                None if args.export is None
                else os.path.join(args.export, str(cnt))
            )

    if not args.monitor_output:
        monitor.write_json("results.json")
        return

    print("\n===> Observing crash directory...")
    observer = Observer()
    observer.schedule(monitor, args.input_dir, recursive=False)
    observer.start()
    try:
        while observer.is_alive():
            observer.join(1)
    except KeyboardInterrupt:
        print("Ending...")
        monitor.write_json("results.json")
    finally:
        observer.stop()
        observer.join()


# Call main
if __name__ == "__main__":
    main()
